{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import sklearn as sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distribution-generator.ipynb:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"C:\\\\Users\\\\ramka\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py:14: SettingWithCopyWarning: \\n\",\n",
      "distribution-generator.ipynb:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"A value is trying to be set on a copy of a slice from a DataFrame.\\n\",\n",
      "distribution-generator.ipynb:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"Try using .loc[row_indexer,col_indexer] = value instead\\n\",\n",
      "distribution-generator.ipynb:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"C:\\\\Users\\\\ramka\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py:16: SettingWithCopyWarning: \\n\",\n"
     ]
    }
   ],
   "source": [
    "%run feature-probability-generation.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = white_wine_new.drop(['quality'], axis=1)\n",
    "y = white_wine_new['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>...</th>\n",
       "      <th>density_prob_low</th>\n",
       "      <th>total_sulfur_dioxide_prob_low</th>\n",
       "      <th>free_sulfur_dioxide_prob_low</th>\n",
       "      <th>chlorides_prob_low</th>\n",
       "      <th>residual_sugar_prob_low</th>\n",
       "      <th>citric_acid_prob_low</th>\n",
       "      <th>volatile_acidity_prob_low</th>\n",
       "      <th>fixed_acidity_prob_low</th>\n",
       "      <th>sulphates_prob_low</th>\n",
       "      <th>pH_prob_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.28</td>\n",
       "      <td>13.55</td>\n",
       "      <td>0.048</td>\n",
       "      <td>51.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.99782</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.4274</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.3002</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.052</td>\n",
       "      <td>40.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>0.3227</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.1423</td>\n",
       "      <td>0.2861</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>8.2</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.36</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.034</td>\n",
       "      <td>12.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.99440</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>0.2110</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.1334</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>5.8</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.043</td>\n",
       "      <td>24.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.99330</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.3298</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>8.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>4.70</td>\n",
       "      <td>0.031</td>\n",
       "      <td>31.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99220</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "3343            6.9              0.36         0.28           13.55      0.048   \n",
       "736             6.6              0.25         0.30           14.40      0.052   \n",
       "1406            8.2              0.22         0.36            6.80      0.034   \n",
       "1719            5.8              0.23         0.27            1.80      0.043   \n",
       "2163            8.5              0.25         0.27            4.70      0.031   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "3343                 51.0                 189.0  0.99782  3.00       0.60   \n",
       "736                  40.0                 183.0  0.99800  3.02       0.50   \n",
       "1406                 12.0                  90.0  0.99440  3.01       0.38   \n",
       "1719                 24.0                  69.0  0.99330  3.38       0.31   \n",
       "2163                 31.0                  92.0  0.99220  3.01       0.33   \n",
       "\n",
       "         ...       density_prob_low  total_sulfur_dioxide_prob_low  \\\n",
       "3343     ...                 0.0039                         0.0257   \n",
       "736      ...                 0.0032                         0.0468   \n",
       "1406     ...                 0.3194                         0.0998   \n",
       "1719     ...                 0.2068                         0.0106   \n",
       "2163     ...                 0.0240                         0.1212   \n",
       "\n",
       "      free_sulfur_dioxide_prob_low  chlorides_prob_low  \\\n",
       "3343                        0.0009              0.4274   \n",
       "736                         0.0076              0.3227   \n",
       "1406                        0.2110              0.0107   \n",
       "1719                        0.2416              0.3298   \n",
       "2163                        0.0904              0.0014   \n",
       "\n",
       "      residual_sugar_prob_low  citric_acid_prob_low  \\\n",
       "3343                   0.0000                0.3060   \n",
       "736                    0.0000                0.3081   \n",
       "1406                   0.0545                0.1334   \n",
       "1719                   0.3184                0.2830   \n",
       "2163                   0.1622                0.2830   \n",
       "\n",
       "      volatile_acidity_prob_low  fixed_acidity_prob_low  sulphates_prob_low  \\\n",
       "3343                     0.2849                  0.3002              0.0018   \n",
       "736                      0.0058                  0.1423              0.2861   \n",
       "1406                     0.0003                  0.0078              0.0051   \n",
       "1719                     0.0009                  0.0005              0.0001   \n",
       "2163                     0.0058                  0.0012              0.0001   \n",
       "\n",
       "      pH_prob_low  \n",
       "3343       0.0002  \n",
       "736        0.0008  \n",
       "1406       0.0005  \n",
       "1719       0.0009  \n",
       "2163       0.0005  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[ 16  44   2]\n",
      " [ 61 954  56]\n",
      " [  2 239  96]]\n",
      "accuracy score is: 0.7251700680272108.\n",
      "For a k value = 3 : \n",
      "[[  7  51   4]\n",
      " [ 23 931 117]\n",
      " [  6 180 151]]\n",
      "accuracy score is: 0.7408163265306122.\n",
      "For a k value = 4 : \n",
      "[[  8  51   3]\n",
      " [ 16 989  66]\n",
      " [  2 227 108]]\n",
      "accuracy score is: 0.7517006802721088.\n",
      "For a k value = 5 : \n",
      "[[  5  52   5]\n",
      " [ 10 962  99]\n",
      " [  1 206 130]]\n",
      "accuracy score is: 0.7462585034013606.\n",
      "For a k value = 6 : \n",
      "[[   4   56    2]\n",
      " [  13 1004   54]\n",
      " [   0  255   82]]\n",
      "accuracy score is: 0.7414965986394558.\n",
      "For a k value = 7 : \n",
      "[[  2  58   2]\n",
      " [  1 994  76]\n",
      " [  0 230 107]]\n",
      "accuracy score is: 0.7503401360544217.\n"
     ]
    }
   ],
   "source": [
    "#Knn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'alcohol_prob', 'pH', 'sulphates', 'citric acid', 'density_prob', 'residual sugar', 'total_sulfur_dioxide_prob', 'free_sulfur_dioxide_prob', 'chlorides_prob', 'residual_sugar_prob', 'citric_acid_prob', 'volatile_acidity_prob', 'fixed_acidity_prob', 'sulphates_prob', 'pH_prob'] ]\n",
    "\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[1, 2, 3]))\n",
    "    print(\"accuracy score is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[1, 2, 3]))/1470) + \".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#knn classification taking features that seem to be strong high quality indicators\\n\\nX_train = X_train[ [\\'alcohol\\', \\'density\\', \\'alcohol_prob\\', \\'density_prob\\', \\'total_sulfur_dioxide_prob\\', \\'free_sulfur_dioxide_prob\\', \\'chlorides_prob\\', \\'residual_sugar_prob\\', \\'citric_acid_prob\\', \\'volatile_acidity_prob\\', \\'fixed_acidity_prob\\', \\'sulphates_prob\\', \\'pH_prob\\'] ]\\n\\nks = [2,3,4,5,6,7]\\n\\nfor k in ks:    \\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    knn.fit(X_train, y_train)\\n    y_pred = knn.predict(X_test)\\n    print(\"For a k value = \" + str(k) + \" : \")\\n    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\\n    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\\n    '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#knn classification taking features that seem to be strong high quality indicators\n",
    "\n",
    "X_train = X_train[ ['alcohol', 'density', 'alcohol_prob', 'density_prob', 'total_sulfur_dioxide_prob', 'free_sulfur_dioxide_prob', 'chlorides_prob', 'residual_sugar_prob', 'citric_acid_prob', 'volatile_acidity_prob', 'fixed_acidity_prob', 'sulphates_prob', 'pH_prob'] ]\n",
    "\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = X_train[ [\\'free sulfur dioxide\\', \\'volatile acidity\\', \\'fixed acidity\\'] ]\\n\\nks = [2,3,4,5,6,7]\\n\\nfor k in ks:    \\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    knn.fit(X_train, y_train)\\n    y_pred = knn.predict(X_test)\\n    print(\"For a k value = \" + str(k) + \" : \")\\n    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\\n    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\\n    '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong low quality indicators\n",
    "\"\"\"\n",
    "X_train = X_train[ ['free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = X_train[ [\\'chlorides\\', \\'residual sugar\\'] ]\\n\\nks = [2,3,4,5,6,7]\\n\\nfor k in ks:    \\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    knn.fit(X_train, y_train)\\n    y_pred = knn.predict(X_test)\\n    print(\"For a k value = \" + str(k) + \" : \")\\n    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\\n    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\\n    '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong all around quality indicators\n",
    "\"\"\"\n",
    "X_train = X_train[ ['chlorides', 'residual sugar'] ]\n",
    "\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best knn classification so far is a 7 neighbor classifier on the 'alcohol' and 'density' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   59    1]\n",
      " [   1 1000   70]\n",
      " [   0  222  115]]\n",
      "accuracy score is: 0.7598639455782313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   61    1]\n",
      " [   0 1051   20]\n",
      " [   0  318   19]]\n",
      "accuracy score is: 0.7278911564625851.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   61    1]\n",
      " [   0 1060   11]\n",
      " [   0  332    5]]\n",
      "accuracy score is: 0.7244897959183674.\n",
      "[[   0   60    2]\n",
      " [   0 1006   65]\n",
      " [   0  239   98]]\n",
      "accuracy score is: 0.7510204081632653.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "\n",
    "solvers = ['newton-cg', 'sag', 'saga', 'lbfgs' ]\n",
    "for solver in solvers:\n",
    "    mulnom_clf = linear_model.LogisticRegression(multi_class='multinomial', solver=solver)\n",
    "    mulnom_clf.fit(X_train, y_train)\n",
    "    y_pred_mulnom = mulnom_clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred_mulnom, labels=[1, 2, 3]))\n",
    "    print(\"accuracy score is: \" + str(np.trace(confusion_matrix(y_test, y_pred_mulnom, labels=[1, 2, 3]))/1470) + \".\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn import linear_model\\n\\n#X_train = X_train[ [\\'alcohol\\', \\'density\\', \\'total sulfur dioxide\\', \\'free sulfur dioxide\\', \\'volatile acidity\\', \\'fixed acidity\\'] ]\\nsolvers = [\\'newton-cg\\', \\'sag\\', \\'saga\\', \\'lbfgs\\' ]\\nfor solver in solvers:\\n    mulnom_clf = linear_model.LogisticRegression(multi_class=\\'multinomial\\', solver=solver)\\n    mulnom_clf.fit(X_train, y_train)\\n    y_pred_mulnom = mulnom_clf.predict(X_test)\\n    print(confusion_matrix(y_test, y_pred_mulnom, labels=[1, 2, 3]))\\n    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_mulnom, labels=[1, 2, 3]))) + \".\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn import linear_model\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "solvers = ['newton-cg', 'sag', 'saga', 'lbfgs' ]\n",
    "for solver in solvers:\n",
    "    mulnom_clf = linear_model.LogisticRegression(multi_class='multinomial', solver=solver)\n",
    "    mulnom_clf.fit(X_train, y_train)\n",
    "    y_pred_mulnom = mulnom_clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred_mulnom, labels=[1, 2, 3]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_mulnom, labels=[1, 2, 3]))) + \".\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train = X_train[ [\\'alcohol\\', \\'density\\', \\'total sulfur dioxide\\', \\'free sulfur dioxide\\', \\'volatile acidity\\', \\'fixed acidity\\'] ]\\n\\nestimatators = [10, 20,  30, 40, 50, 60, 70, 80, 90, 100 ]\\nfor estimator in estimatators:\\n    grad_boost_clf = sklearn.ensemble.GradientBoostingClassifier(n_estimators=estimator)\\n    grad_boost_clf.fit(X_train, y_train)\\n    y_pred_grad_boost = grad_boost_clf.predict(X_test)\\n    print(\"For n_estimators = \" + str(estimator) + \".\")\\n    print(confusion_matrix(y_test, y_pred_grad_boost, labels=[3, 4, 5, 6, 7, 8, 9]))\\n    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_grad_boost, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\\n    print(\"---------------------------------------------------------------------------------------------------------\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "\n",
    "estimatators = [10, 20,  30, 40, 50, 60, 70, 80, 90, 100 ]\n",
    "for estimator in estimatators:\n",
    "    grad_boost_clf = sklearn.ensemble.GradientBoostingClassifier(n_estimators=estimator)\n",
    "    grad_boost_clf.fit(X_train, y_train)\n",
    "    y_pred_grad_boost = grad_boost_clf.predict(X_test)\n",
    "    print(\"For n_estimators = \" + str(estimator) + \".\")\n",
    "    print(confusion_matrix(y_test, y_pred_grad_boost, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_grad_boost, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forrest has perfromed the best out of all the classifiers. I will continue to use random forrest and tweak it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "\n",
    "\n",
    "estimators = list(range(50, 3000, 100))\n",
    "points = {}\n",
    "for estimator in estimators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    points[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3])) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the more features I have the better the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(points.keys(), points.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.ensemble import RandomForestClassifier\\n\\n#X_train = X_train[ ['alcohol', 'density'] ]\\n\\nestimators = list(range(50, 3000, 100))\\npoints_high_indicators = {}\\nfor estimator in estimators:\\n    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\\n    ran_fores_clf.fit(X_train, y_train)\\n    y_pred_ran_fores = ran_fores_clf.predict(X_test)\\n    points_high_indicators[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])))/980\\n    \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density'] ]\n",
    "\n",
    "estimators = list(range(50, 3000, 100))\n",
    "points_high_indicators = {}\n",
    "for estimator in estimators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    points_high_indicators[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])))/980\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure()\\nplt.scatter(points_high_indicators.keys(), points_high_indicators.values())\\nplt.show()'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.figure()\n",
    "plt.scatter(points_high_indicators.keys(), points_high_indicators.values())\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.ensemble import RandomForestClassifier\\n\\nX_train = X_train[ ['free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\\n\\nestimators = list(range(50, 3000, 100))\\npoints_low_indicators = {}\\nfor estimator in estimators:\\n    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\\n    ran_fores_clf.fit(X_train, y_train)\\n    y_pred_ran_fores = ran_fores_clf.predict(X_test)\\n    points_low_indicators[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])) )/980\\n    \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train = X_train[ ['free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "\n",
    "estimators = list(range(50, 3000, 100))\n",
    "points_low_indicators = {}\n",
    "for estimator in estimators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    points_low_indicators[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])) )/980\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure()\\nplt.scatter(points_low_indicators.keys(), points_low_indicators.values())\\nplt.show()'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.figure()\n",
    "plt.scatter(points_low_indicators.keys(), points_low_indicators.values())\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.ensemble import RandomForestClassifier\\n\\nX_train = X_train[ ['chlorides', 'residual sugar'] ]\\n\\nestimators = list(range(50, 3000, 100))\\npoints_three_indicators = {}\\nfor estimator in estimators:\\n    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\\n    ran_fores_clf.fit(X_train, y_train)\\n    y_pred_ran_fores = ran_fores_clf.predict(X_test)\\n    points_three_indicators[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])) )/980\\n    \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train = X_train[ ['chlorides', 'residual sugar'] ]\n",
    "\n",
    "estimators = list(range(50, 3000, 100))\n",
    "points_three_indicators = {}\n",
    "for estimator in estimators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    points_three_indicators[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])) )/980\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure()\\nplt.scatter(points_three_indicators.keys(), points_three_indicators.values())\\nplt.show()'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.figure()\n",
    "plt.scatter(points_three_indicators.keys(), points_three_indicators.values())\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.ensemble import RandomForestClassifier\\n\\nX_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'chlorides', 'residual sugar'] ]\\n\\nestimators = list(range(50, 3000, 100))\\npoints_all = {}\\nfor estimator in estimators:\\n    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\\n    ran_fores_clf.fit(X_train, y_train)\\n    y_pred_ran_fores = ran_fores_clf.predict(X_test)\\n    points_all[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])) )/980\\n    \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'chlorides', 'residual sugar'] ]\n",
    "\n",
    "estimators = list(range(50, 3000, 100))\n",
    "points_all = {}\n",
    "for estimator in estimators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    points_all[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9])) )/980\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plt.figure()\\nplt.scatter(points_all.keys(), points_all.values())\\nplt.show()'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"plt.figure()\n",
    "plt.scatter(points_all.keys(), points_all.values())\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1150 estimators seems to be the optimal estimator amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For max_feature parameter 'sqrt' we have score 0.8340136054421768.\n",
      "For max_feature parameter 'log2' we have score 0.8340136054421768.\n",
      "For max_feature parameter 'auto' we have score 0.8353741496598639.\n"
     ]
    }
   ],
   "source": [
    "#max_feature parameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'chlorides', 'residual sugar'] ]\n",
    "\n",
    "parameters = ['sqrt', 'log2', 'auto']\n",
    "for param in parameters:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=1150, max_features=param)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    print( \"For max_feature parameter '\" + str(param) + \"' we have score \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best max_feature parameter is 'auto'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For criterion parameter 'gini' we have score 0.8326530612244898.\n",
      "For criterion parameter 'entropy' we have score 0.8353741496598639.\n"
     ]
    }
   ],
   "source": [
    "#criterion parameter tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'chlorides', 'residual sugar'] ]\n",
    "\n",
    "parameters = ['gini', 'entropy']\n",
    "for param in parameters:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=1150, max_features='auto', criterion=param )\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    print( \"For criterion parameter '\" + str(param) + \"' we have score \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best criterion parameter is 'entropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For bootstrap parameter 'True' we have score 0.8360544217687075.\n",
      "For bootstrap parameter 'False' we have score 0.8414965986394558.\n"
     ]
    }
   ],
   "source": [
    "#bootstrap  parameter tuning\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'chlorides', 'residual sugar'] ]\n",
    "\n",
    "parameters = [True, False]\n",
    "for param in parameters:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=1150, max_features='auto', criterion='entropy', bootstrap=param )\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    print( \"For bootstrap parameter '\" + str(param) + \"' we have score \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best bootstrap parameter is 'False'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My best classifier yeilds an accuracy of 0.8428571428571429.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#X_train = X_train[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity', 'chlorides', 'residual sugar'] ]\n",
    "\n",
    "ran_fores_clf = RandomForestClassifier(n_estimators=1450, max_features='auto', criterion='entropy', bootstrap=False )\n",
    "ran_fores_clf.fit(X_train, y_train)\n",
    "y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "print( \"My best classifier yeilds an accuracy of \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimators = list(range(50, 3000, 100))\n",
    "points = {}\n",
    "for estimator in estimators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator, max_features='auto', criterion='entropy', bootstrap=False )\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    points[estimator] = ( np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(points.keys(), points.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1241"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7591836734693878.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train_high = X_train[ ['alcohol', 'alcohol_prob_high', 'density', 'density_prob_high', 'density_prob_mid', 'density_prob_low', 'alcohol_prob_mid', 'alcohol_prob_low' ] ]\n",
    "X_test_high = X_test[ ['alcohol', 'alcohol_prob_high', 'density', 'density_prob_high', 'density_prob_mid', 'density_prob_low', 'alcohol_prob_mid', 'alcohol_prob_low' ] ]\n",
    "\n",
    "ran_fores_clf = RandomForestClassifier(n_estimators=1450, max_features='auto', criterion='entropy', bootstrap=False )\n",
    "ran_fores_clf.fit(X_train_high, y_train)\n",
    "y_pred_ran_fores = ran_fores_clf.predict(X_test_high)\n",
    "print( \"accuracy \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7285714285714285.\n"
     ]
    }
   ],
   "source": [
    " from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train_low = X_train[ ['chlorides', 'chlorides_prob_low', 'chlorides_prob_mid', 'chlorides_prob_high', 'residual sugar', 'residual_sugar_prob_low', 'residual_sugar_prob_mid', 'residual_sugar_prob_high'] ]\n",
    "X_test_low = X_test[ ['chlorides', 'chlorides_prob_low', 'chlorides_prob_mid', 'chlorides_prob_high', 'residual sugar', 'residual_sugar_prob_low', 'residual_sugar_prob_mid', 'residual_sugar_prob_high'] ]\n",
    "\n",
    "ran_fores_clf = RandomForestClassifier(n_estimators=1450, max_features='auto', criterion='entropy', bootstrap=False )\n",
    "ran_fores_clf.fit(X_train_low, y_train)\n",
    "y_pred_ran_fores = ran_fores_clf.predict(X_test_low)\n",
    "print( \"accuracy \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7646258503401361.\n"
     ]
    }
   ],
   "source": [
    " from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train_lo = X_train[ ['volatile acidity', 'volatile_acidity_prob_low', 'volatile_acidity_prob_mid', 'volatile_acidity_prob_high', 'fixed acidity', 'fixed_acidity_prob_low', 'fixed_acidity_prob_mid', 'fixed_acidity_prob_high', 'free sulfur dioxide', 'free_sulfur_dioxide_prob_low', 'free_sulfur_dioxide_prob_mid', 'free_sulfur_dioxide_prob_high'] ]\n",
    "X_test_lo = X_test[ ['volatile acidity', 'volatile_acidity_prob_low', 'volatile_acidity_prob_mid', 'volatile_acidity_prob_high', 'fixed acidity', 'fixed_acidity_prob_low', 'fixed_acidity_prob_mid', 'fixed_acidity_prob_high', 'free sulfur dioxide', 'free_sulfur_dioxide_prob_low', 'free_sulfur_dioxide_prob_mid', 'free_sulfur_dioxide_prob_high'] ]\n",
    "\n",
    "ran_fores_clf = RandomForestClassifier(n_estimators=1450, max_features='auto', criterion='entropy', bootstrap=False )\n",
    "ran_fores_clf.fit(X_train_lo, y_train)\n",
    "y_pred_ran_fores = ran_fores_clf.predict(X_test_lo)\n",
    "print( \"accuracy \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[1, 2, 3]))/1470) + \".\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
