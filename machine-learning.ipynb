{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import sklearn as sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "red_wine = pd.read_csv('https://query.data.world/s/rrj4d2f6sj2apz4x7sozpd6epmrsiy')\n",
    "white_wine = pd.read_csv('https://query.data.world/s/brho7trfnuy4iz2nxraqzsfwvgek5b')\n",
    "\n",
    "high_quality = white_wine.loc[ white_wine['quality'] > 6 ]   #ratings 9, 8, 7 only, 1060 rows\n",
    "mid_quality_initial = white_wine.loc[ white_wine['quality'] < 7 ]\n",
    "mid_quality = mid_quality_initial.loc[ mid_quality_initial['quality'] > 4 ]   #ratings 5, 6 only, 3655 rows\n",
    "low_quality = white_wine.loc[ white_wine['quality'] < 5 ]   #ratings 3, 4 only, 183 rows\n",
    "mid_and_low_quality = white_wine.loc[ white_wine['quality'] < 7 ]   #ratings 3, 4, 5, 6 only, 3838 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  1   0   2   1   1   0   0]\n",
      " [  2   9   9   4   1   0   0]\n",
      " [  2  11 205  69   3   1   0]\n",
      " [  0  17 165 212  32   6   0]\n",
      " [  2   8  40  81  59   2   0]\n",
      " [  0   1   4  18   8   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 490.\n",
      "For a k value = 3 : \n",
      "[[  1   0   2   1   1   0   0]\n",
      " [  2   3  11   7   2   0   0]\n",
      " [  1  12 156  97  21   4   0]\n",
      " [  0  11 137 221  53  10   0]\n",
      " [  0  10  40  69  66   7   0]\n",
      " [  0   1   4  17   6   7   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 454.\n",
      "For a k value = 4 : \n",
      "[[  1   0   3   0   1   0   0]\n",
      " [  1   0  14   8   2   0   0]\n",
      " [  0   7 172  87  22   3   0]\n",
      " [  0   5 128 240  49  10   0]\n",
      " [  0   3  35  87  61   6   0]\n",
      " [  0   1   2  19   7   6   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 480.\n",
      "For a k value = 5 : \n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  1   2  13   7   2   0   0]\n",
      " [  0   6 149 116  19   1   0]\n",
      " [  0   5 112 261  50   4   0]\n",
      " [  0   3  44  89  51   5   0]\n",
      " [  0   0   1  21   9   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 467.\n",
      "For a k value = 6 : \n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  1   3  12   7   2   0   0]\n",
      " [  0   4 161 107  18   1   0]\n",
      " [  0   5 129 253  43   2   0]\n",
      " [  0   4  44  92  48   4   0]\n",
      " [  0   0   2  21   8   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 469.\n",
      "For a k value = 7 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   5  10   8   2   0   0]\n",
      " [  0   3 140 124  21   3   0]\n",
      " [  0   4 105 272  49   2   0]\n",
      " [  0   2  34 102  49   5   0]\n",
      " [  0   0   3  24   6   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 468.\n"
     ]
    }
   ],
   "source": [
    "#Knn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "X_initial = white_wine.drop(['quality'], axis=1)\n",
    "X = X_initial[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  0   2   2   1   0   0   0]\n",
      " [  2   5   8   7   2   1   0]\n",
      " [  1  20 202  58   9   1   0]\n",
      " [  3  23 149 225  30   2   0]\n",
      " [  0  18  30  83  59   2   0]\n",
      " [  0   1   3  16   8   7   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 498.\n",
      "For a k value = 3 : \n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  2   3   9   5   5   1   0]\n",
      " [  1  15 154 103  17   1   0]\n",
      " [  1  22 106 243  55   5   0]\n",
      " [  0  15  28  66  79   4   0]\n",
      " [  0   0   1  15  11   8   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 487.\n",
      "For a k value = 4 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  1   2  10   9   3   0   0]\n",
      " [  0   8 168  93  20   2   0]\n",
      " [  1  12 118 252  44   5   0]\n",
      " [  0   4  24  87  74   3   0]\n",
      " [  0   0   2  14  12   7   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 503.\n",
      "For a k value = 5 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  1   1   7  11   5   0   0]\n",
      " [  0   7 172 100  12   0   0]\n",
      " [  0   5 118 264  40   5   0]\n",
      " [  0   3  24  96  67   2   0]\n",
      " [  0   0   1  19  12   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 507.\n",
      "For a k value = 6 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   0   9  14   2   0   0]\n",
      " [  0   6 178  91  16   0   0]\n",
      " [  0   2 117 265  43   5   0]\n",
      " [  0   2  20 107  63   0   0]\n",
      " [  0   1   3  17  11   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 509.\n",
      "For a k value = 7 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   0   8  15   2   0   0]\n",
      " [  0   3 170 101  17   0   0]\n",
      " [  0   1 104 281  42   4   0]\n",
      " [  0   1  23 108  59   1   0]\n",
      " [  0   1   3  17  10   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 514.\n"
     ]
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong high quality indicators\n",
    "X = X_initial[ ['alcohol', 'density'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  1   8   8   8   0   0   0]\n",
      " [  3  16 186  72  12   2   0]\n",
      " [  2  24 164 219  21   2   0]\n",
      " [  0   3  79  69  41   0   0]\n",
      " [  0   0  12  11   8   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 459.\n",
      "For a k value = 3 : \n",
      "[[  0   0   3   1   1   0   0]\n",
      " [  1   3  11   9   1   0   0]\n",
      " [  3  14 145 102  22   5   0]\n",
      " [  1  16 122 247  44   2   0]\n",
      " [  0   1  72  65  52   2   0]\n",
      " [  0   0  13  12   5   5   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 452.\n",
      "For a k value = 4 : \n",
      "[[  1   0   2   1   1   0   0]\n",
      " [  1   2  14   7   1   0   0]\n",
      " [  0   8 149 108  25   1   0]\n",
      " [  1   7 127 260  33   4   0]\n",
      " [  0   2  69  73  46   2   0]\n",
      " [  0   0  12  14   4   5   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 463.\n",
      "For a k value = 5 : \n",
      "[[  1   0   3   1   0   0   0]\n",
      " [  1   3  14   7   0   0   0]\n",
      " [  0   9 137 126  18   1   0]\n",
      " [  0   8 134 257  28   5   0]\n",
      " [  0   2  70  81  38   1   0]\n",
      " [  0   0  10  16   6   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 439.\n",
      "For a k value = 6 : \n",
      "[[  1   0   1   2   1   0   0]\n",
      " [  1   3  12   9   0   0   0]\n",
      " [  0   8 138 120  24   1   0]\n",
      " [  0  10 127 264  28   3   0]\n",
      " [  0   3  72  81  35   1   0]\n",
      " [  0   1  12  14   5   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 444.\n",
      "For a k value = 7 : \n",
      "[[  1   0   1   2   1   0   0]\n",
      " [  1   3  12   8   1   0   0]\n",
      " [  0   7 137 119  27   1   0]\n",
      " [  0   9 127 262  33   1   0]\n",
      " [  0   1  61  91  37   2   0]\n",
      " [  0   0  11  17   4   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 443.\n"
     ]
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong low quality indicators\n",
    "X = X_initial[ ['free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  0   0   3   1   1   0   0]\n",
      " [  0   2  11  11   1   0   0]\n",
      " [  0  27 173  75  15   1   0]\n",
      " [  3  24 171 199  28   7   0]\n",
      " [  3  10  42  86  47   4   0]\n",
      " [  0   1   7  15   9   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 424.\n",
      "For a k value = 3 : \n",
      "[[  0   0   2   1   2   0   0]\n",
      " [  0   2   9  12   2   0   0]\n",
      " [  2  23 126 115  22   3   0]\n",
      " [  2  20 141 218  43   8   0]\n",
      " [  3  11  37  73  62   6   0]\n",
      " [  0   1   8  12  10   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 412.\n",
      "For a k value = 4 : \n",
      "[[  0   0   2   2   1   0   0]\n",
      " [  0   1  10  10   3   1   0]\n",
      " [  0  13 147 109  18   4   0]\n",
      " [  0  12 142 230  43   5   0]\n",
      " [  0   6  35  94  52   5   0]\n",
      " [  0   1   7  12  11   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 434.\n",
      "For a k value = 5 : \n",
      "[[  0   0   2   2   1   0   0]\n",
      " [  0   0  10  11   4   0   0]\n",
      " [  0   6 138 130  13   4   0]\n",
      " [  1   6 134 254  33   4   0]\n",
      " [  0   0  32 105  49   6   0]\n",
      " [  0   0   6  15  10   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 445.\n",
      "For a k value = 6 : \n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  0   1  10  12   2   0   0]\n",
      " [  0   3 137 138  11   2   0]\n",
      " [  0   5 146 242  35   4   0]\n",
      " [  0   0  33 103  52   4   0]\n",
      " [  0   0   7  18   8   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 434.\n",
      "For a k value = 7 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   1  10  11   2   1   0]\n",
      " [  0   1 129 148  12   1   0]\n",
      " [  0   4 129 259  38   2   0]\n",
      " [  0   0  30 107  51   4   0]\n",
      " [  0   0   7  18   8   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 442.\n"
     ]
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong all around quality indicators\n",
    "X = X_initial[ ['chlorides', 'residual sugar'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best knn classification so far is a 7 neighbor classifier on the 'alcohol' and 'density' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   2   2   0   1   0]\n",
      " [  0   0  14  11   0   0   0]\n",
      " [  0   0 138 151   2   0   0]\n",
      " [  1   0  91 320  20   0   0]\n",
      " [  0   0  18 138  36   0   0]\n",
      " [  0   0   2  21  12   0   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 494.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   0   8  16   1   0   0]\n",
      " [  0   0  88 202   1   0   0]\n",
      " [  0   0  74 355   3   0   0]\n",
      " [  0   0  17 164  11   0   0]\n",
      " [  0   0   2  29   4   0   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 454.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   0   7  17   1   0   0]\n",
      " [  0   0  84 205   2   0   0]\n",
      " [  0   0  76 353   3   0   0]\n",
      " [  0   0  17 169   6   0   0]\n",
      " [  0   0   2  32   1   0   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 443.\n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  0   0   7  18   0   0   0]\n",
      " [  0   0  88 202   1   0   0]\n",
      " [  0   0  75 356   1   0   0]\n",
      " [  0   0  18 170   4   0   0]\n",
      " [  0   0   2  33   0   0   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 449.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "X_initial = white_wine.drop(['quality'], axis=1)\n",
    "X = X_initial[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "solvers = ['newton-cg', 'sag', 'saga', 'lbfgs' ]\n",
    "for solver in solvers:\n",
    "    mulnom_clf = linear_model.LogisticRegression(multi_class='multinomial', solver=solver)\n",
    "    mulnom_clf.fit(X_train, y_train)\n",
    "    y_pred_mulnom = mulnom_clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred_mulnom, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_mulnom, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1   0   0   2   0   0]\n",
      " [  0  16   0   0  11   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   3   0   0 176   0   0]\n",
      " [  0   0   0   0  39   0   0]\n",
      " [  0   0   0   0   1   0   0]]\n",
      "Sum of the diagonals is: 192.\n",
      "[[  0   0   0   0   3   0   0]\n",
      " [  0   5   0   0  22   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   2   0   0 177   0   0]\n",
      " [  0   0   0   0  39   0   0]\n",
      " [  0   0   0   0   1   0   0]]\n",
      "Sum of the diagonals is: 182.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   3   0   0]\n",
      " [  0   5   0   0  22   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   2   0   0 177   0   0]\n",
      " [  0   0   0   0  39   0   0]\n",
      " [  0   0   0   0   1   0   0]]\n",
      "Sum of the diagonals is: 182.\n",
      "[[  0   1   0   0   2   0   0]\n",
      " [  0   9   0   0  18   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0]\n",
      " [  0   2   0   0 177   0   0]\n",
      " [  0   0   0   0  39   0   0]\n",
      " [  0   0   0   0   1   0   0]]\n",
      "Sum of the diagonals is: 186.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramka\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "high_and_low = high_quality.append(low_quality)\n",
    "X_initial = high_and_low.drop(['quality'], axis=1)\n",
    "X = X_initial[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = high_and_low['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "solvers = ['newton-cg', 'sag', 'saga', 'lbfgs' ]\n",
    "for solver in solvers:\n",
    "    mulnom_clf = linear_model.LogisticRegression(multi_class='multinomial', solver=solver)\n",
    "    mulnom_clf.fit(X_train, y_train)\n",
    "    y_pred_mulnom = mulnom_clf.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred_mulnom, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_mulnom, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators = 2.\n",
      "[[  0   1   3   1   0   0   0]\n",
      " [  3   9  10   3   0   0   0]\n",
      " [  2  20 203  62   4   0   0]\n",
      " [  0  20 133 246  33   0   0]\n",
      " [  0   5  32  81  71   3   0]\n",
      " [  0   2   4  12   5  12   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 541.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 3.\n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  0   7  12   6   0   0   0]\n",
      " [  2  20 189  73   7   0   0]\n",
      " [  1   9 104 266  48   4   0]\n",
      " [  0   8  23  61  97   3   0]\n",
      " [  0   2   2  11   5  15   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 574.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 4.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   6  13   6   0   0   0]\n",
      " [  1  17 198  68   6   1   0]\n",
      " [  0   7 105 286  30   4   0]\n",
      " [  0   2  18  67  99   5   1]\n",
      " [  0   1   1   9   8  16   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 605.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 5.\n",
      "[[  1   0   4   0   0   0   0]\n",
      " [  0   8  11   6   0   0   0]\n",
      " [  0   7 196  85   3   0   0]\n",
      " [  0   5  85 298  43   1   0]\n",
      " [  0   1  11  76  99   5   0]\n",
      " [  0   0   1  10  14  10   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 612.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 6.\n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  1   8  11   5   0   0   0]\n",
      " [  0   9 208  66   8   0   0]\n",
      " [  0   5 100 289  37   1   0]\n",
      " [  0   1  14  84  92   1   0]\n",
      " [  0   0   2  11   8  13   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 610.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 7.\n",
      "[[  0   0   1   4   0   0   0]\n",
      " [  0   4  13   8   0   0   0]\n",
      " [  0   4 201  76  10   0   0]\n",
      " [  0   4  79 312  36   1   0]\n",
      " [  0   1   6  73 107   5   0]\n",
      " [  0   0   1  13   7  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 638.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 8.\n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   7   9   9   0   0   0]\n",
      " [  0   4 204  79   4   0   0]\n",
      " [  0   1  87 304  36   3   1]\n",
      " [  0   0  10  77  99   6   0]\n",
      " [  0   0   0  17   4  13   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 627.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 9.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  1   6   9   9   0   0   0]\n",
      " [  0   6 199  84   2   0   0]\n",
      " [  0   0  78 322  28   4   0]\n",
      " [  0   1   4  76 108   3   0]\n",
      " [  0   0   0  12   7  15   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 650.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 10.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   5  15   5   0   0   0]\n",
      " [  0   5 202  78   6   0   0]\n",
      " [  0   1  77 308  42   3   1]\n",
      " [  0   0   6  72 110   4   0]\n",
      " [  0   0   0  12   8  14   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 639.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 11.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   7  11   6   1   0   0]\n",
      " [  0   6 211  70   4   0   0]\n",
      " [  0   1  88 314  29   0   0]\n",
      " [  0   1   6  75 107   3   0]\n",
      " [  0   0   1  13   4  16   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 655.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 12.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  1   6  13   5   0   0   0]\n",
      " [  0   3 193  92   3   0   0]\n",
      " [  0   0  77 316  37   2   0]\n",
      " [  0   0   9  76 105   2   0]\n",
      " [  0   0   0  13   7  14   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 634.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 13.\n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   8  12   5   0   0   0]\n",
      " [  0   3 196  83   9   0   0]\n",
      " [  0   4  69 331  27   1   0]\n",
      " [  0   0   4  76 109   3   0]\n",
      " [  0   0   1  14   6  13   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 657.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 14.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   5  15   5   0   0   0]\n",
      " [  0   9 202  72   8   0   0]\n",
      " [  0   1  79 315  34   3   0]\n",
      " [  0   0   6  67 117   2   0]\n",
      " [  0   0   1   8  11  15   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 654.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 20.\n",
      "[[  0   0   3   1   1   0   0]\n",
      " [  0   7  12   6   0   0   0]\n",
      " [  0   4 198  87   2   0   0]\n",
      " [  0   1  65 332  32   2   0]\n",
      " [  0   1   3  75 111   2   0]\n",
      " [  0   0   1  11   9  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 662.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 30.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   7  10   8   0   0   0]\n",
      " [  0   4 199  84   4   0   0]\n",
      " [  0   1  59 340  32   0   0]\n",
      " [  0   0   3  75 110   4   0]\n",
      " [  0   0   0  10  10  14   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 670.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 40.\n",
      "[[  0   0   1   4   0   0   0]\n",
      " [  0   7  12   6   0   0   0]\n",
      " [  0   4 197  86   4   0   0]\n",
      " [  0   1  71 325  33   2   0]\n",
      " [  0   0   5  71 115   1   0]\n",
      " [  0   0   1  10  10  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 658.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 50.\n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   6  13   6   0   0   0]\n",
      " [  0   3 198  86   4   0   0]\n",
      " [  0   2  58 341  31   0   0]\n",
      " [  0   0   3  80 104   5   0]\n",
      " [  0   0   0  12   9  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 663.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 60.\n",
      "[[  0   0   1   4   0   0   0]\n",
      " [  0   6  12   7   0   0   0]\n",
      " [  0   2 200  86   3   0   0]\n",
      " [  0   0  69 330  33   0   0]\n",
      " [  0   0   6  72 111   3   0]\n",
      " [  0   0   1  11   9  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 661.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 70.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   6  12   7   0   0   0]\n",
      " [  0   3 202  83   3   0   0]\n",
      " [  0   1  67 329  34   1   0]\n",
      " [  0   0   2  77 109   4   0]\n",
      " [  0   0   0  10  10  14   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 660.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 80.\n",
      "[[  0   0   1   4   0   0   0]\n",
      " [  0   6  12   7   0   0   0]\n",
      " [  0   3 193  90   5   0   0]\n",
      " [  0   0  66 335  30   1   0]\n",
      " [  0   0   4  75 111   2   0]\n",
      " [  0   0   0   8  12  15   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 660.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 90.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   7  11   7   0   0   0]\n",
      " [  0   4 195  88   4   0   0]\n",
      " [  0   0  65 336  30   1   0]\n",
      " [  0   0   3  73 115   1   0]\n",
      " [  0   0   0  10  11  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 667.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 100.\n",
      "[[  0   0   1   4   0   0   0]\n",
      " [  0   6  13   6   0   0   0]\n",
      " [  0   2 204  82   3   0   0]\n",
      " [  0   1  63 331  36   1   0]\n",
      " [  0   0   2  76 111   3   0]\n",
      " [  0   0   0  10  11  14   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 666.\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_initial = white_wine.drop(['quality'], axis=1)\n",
    "X = X_initial[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "estimatators = [2, 3,  4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 20, 30, 40, 50, 60, 70, 80, 90, 100 ]\n",
    "for estimator in estimatators:\n",
    "    ran_fores_clf = RandomForestClassifier(n_estimators=estimator)\n",
    "    ran_fores_clf.fit(X_train, y_train)\n",
    "    y_pred_ran_fores = ran_fores_clf.predict(X_test)\n",
    "    print(\"For n_estimators = \" + str(estimator) + \".\")\n",
    "    print(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_ran_fores, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_estimators = 10.\n",
      "[[  0   0   0   5   0   0   0]\n",
      " [  0   3  10  12   0   0   0]\n",
      " [  0   2 144 144   1   0   0]\n",
      " [  0   0  91 322  18   0   1]\n",
      " [  0   0   6 144  35   5   2]\n",
      " [  0   0   0  26   8   1   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 505.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 20.\n",
      "[[  0   0   0   5   0   0   0]\n",
      " [  0   3  14   8   0   0   0]\n",
      " [  0   2 151 135   2   1   0]\n",
      " [  0   2  92 318  19   0   1]\n",
      " [  0   0   7 132  47   4   2]\n",
      " [  0   0   1  21  12   1   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 520.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 30.\n",
      "[[  0   0   0   5   0   0   0]\n",
      " [  0   3  14   8   0   0   0]\n",
      " [  0   3 155 127   5   1   0]\n",
      " [  0   1  94 316  20   0   1]\n",
      " [  0   0   7 128  50   5   2]\n",
      " [  0   0   1  19  12   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 527.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 40.\n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  0   3  14   8   0   0   0]\n",
      " [  1   2 158 126   3   1   0]\n",
      " [  0   2  94 316  19   0   1]\n",
      " [  0   0   8 125  52   5   2]\n",
      " [  0   0   1  20  12   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 532.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 50.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   3  14   8   0   0   0]\n",
      " [  0   4 159 123   4   1   0]\n",
      " [  0   1  92 312  25   1   1]\n",
      " [  0   0   7 121  58   4   2]\n",
      " [  0   0   1  18  13   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 535.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 60.\n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  0   3  14   8   0   0   0]\n",
      " [  0   6 158 122   4   1   0]\n",
      " [  0   2  92 311  26   0   1]\n",
      " [  0   0   7 118  61   4   2]\n",
      " [  0   0   1  17  14   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 537.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 70.\n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  0   3  14   8   0   0   0]\n",
      " [  0   6 158 123   3   1   0]\n",
      " [  0   2  91 310  27   1   1]\n",
      " [  0   0   7 118  61   4   2]\n",
      " [  0   0   1  16  13   5   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 538.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 80.\n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  0   4  13   8   0   0   0]\n",
      " [  0   6 159 121   4   1   0]\n",
      " [  1   2  92 309  26   1   1]\n",
      " [  0   0   6 119  61   4   2]\n",
      " [  0   0   1  16  13   5   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 539.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 90.\n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   4  12   9   0   0   0]\n",
      " [  0   6 159 122   3   1   0]\n",
      " [  0   2  93 308  27   1   1]\n",
      " [  0   1   5 118  62   4   2]\n",
      " [  0   0   1  18  12   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 537.\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "For n_estimators = 100.\n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  0   4  12   9   0   0   0]\n",
      " [  0   6 158 123   3   1   0]\n",
      " [  0   2  94 306  28   1   1]\n",
      " [  0   1   5 118  62   4   2]\n",
      " [  0   0   1  17  11   6   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 536.\n",
      "---------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_initial = white_wine.drop(['quality'], axis=1)\n",
    "X = X_initial[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "estimatators = [10, 20,  30, 40, 50, 60, 70, 80, 90, 100 ]\n",
    "for estimator in estimatators:\n",
    "    grad_boost_clf = sklearn.ensemble.GradientBoostingClassifier(n_estimators=estimator)\n",
    "    grad_boost_clf.fit(X_train, y_train)\n",
    "    y_pred_grad_boost = grad_boost_clf.predict(X_test)\n",
    "    print(\"For n_estimators = \" + str(estimator) + \".\")\n",
    "    print(confusion_matrix(y_test, y_pred_grad_boost, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred_grad_boost, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forrest has perfromed the best out of all the classifiers. I will continue to use random forrest and tweak it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
