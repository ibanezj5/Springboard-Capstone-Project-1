{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "\n",
    "red_wine = pd.read_csv('https://query.data.world/s/rrj4d2f6sj2apz4x7sozpd6epmrsiy')\n",
    "white_wine = pd.read_csv('https://query.data.world/s/brho7trfnuy4iz2nxraqzsfwvgek5b')\n",
    "\n",
    "high_quality = white_wine.loc[ white_wine['quality'] > 6 ]   #ratings 9, 8, 7 only, 1060 rows\n",
    "mid_quality_initial = white_wine.loc[ white_wine['quality'] < 7 ]\n",
    "mid_quality = mid_quality_initial.loc[ mid_quality_initial['quality'] > 4 ]   #ratings 5, 6 only, 3655 rows\n",
    "low_quality = white_wine.loc[ white_wine['quality'] < 5 ]   #ratings 3, 4 only, 183 rows\n",
    "mid_and_low_quality = white_wine.loc[ white_wine['quality'] < 7 ]   #ratings 3, 4, 5, 6 only, 3838 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  1   0   2   1   1   0   0]\n",
      " [  2   9   9   4   1   0   0]\n",
      " [  2  11 205  69   3   1   0]\n",
      " [  0  17 165 212  32   6   0]\n",
      " [  2   8  40  81  59   2   0]\n",
      " [  0   1   4  18   8   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 490.\n",
      "For a k value = 3 : \n",
      "[[  1   0   2   1   1   0   0]\n",
      " [  2   3  11   7   2   0   0]\n",
      " [  1  12 156  97  21   4   0]\n",
      " [  0  11 137 221  53  10   0]\n",
      " [  0  10  40  69  66   7   0]\n",
      " [  0   1   4  17   6   7   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 454.\n",
      "For a k value = 4 : \n",
      "[[  1   0   3   0   1   0   0]\n",
      " [  1   0  14   8   2   0   0]\n",
      " [  0   7 172  87  22   3   0]\n",
      " [  0   5 128 240  49  10   0]\n",
      " [  0   3  35  87  61   6   0]\n",
      " [  0   1   2  19   7   6   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 480.\n",
      "For a k value = 5 : \n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  1   2  13   7   2   0   0]\n",
      " [  0   6 149 116  19   1   0]\n",
      " [  0   5 112 261  50   4   0]\n",
      " [  0   3  44  89  51   5   0]\n",
      " [  0   0   1  21   9   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 467.\n",
      "For a k value = 6 : \n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  1   3  12   7   2   0   0]\n",
      " [  0   4 161 107  18   1   0]\n",
      " [  0   5 129 253  43   2   0]\n",
      " [  0   4  44  92  48   4   0]\n",
      " [  0   0   2  21   8   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 469.\n",
      "For a k value = 7 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   5  10   8   2   0   0]\n",
      " [  0   3 140 124  21   3   0]\n",
      " [  0   4 105 272  49   2   0]\n",
      " [  0   2  34 102  49   5   0]\n",
      " [  0   0   3  24   6   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 468.\n"
     ]
    }
   ],
   "source": [
    "#Knn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_initial = white_wine.drop(['quality'], axis=1)\n",
    "X = X_initial[ ['alcohol', 'density', 'total sulfur dioxide', 'free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  0   2   2   1   0   0   0]\n",
      " [  2   5   8   7   2   1   0]\n",
      " [  1  20 202  58   9   1   0]\n",
      " [  3  23 149 225  30   2   0]\n",
      " [  0  18  30  83  59   2   0]\n",
      " [  0   1   3  16   8   7   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 498.\n",
      "For a k value = 3 : \n",
      "[[  0   0   2   3   0   0   0]\n",
      " [  2   3   9   5   5   1   0]\n",
      " [  1  15 154 103  17   1   0]\n",
      " [  1  22 106 243  55   5   0]\n",
      " [  0  15  28  66  79   4   0]\n",
      " [  0   0   1  15  11   8   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 487.\n",
      "For a k value = 4 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  1   2  10   9   3   0   0]\n",
      " [  0   8 168  93  20   2   0]\n",
      " [  1  12 118 252  44   5   0]\n",
      " [  0   4  24  87  74   3   0]\n",
      " [  0   0   2  14  12   7   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 503.\n",
      "For a k value = 5 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  1   1   7  11   5   0   0]\n",
      " [  0   7 172 100  12   0   0]\n",
      " [  0   5 118 264  40   5   0]\n",
      " [  0   3  24  96  67   2   0]\n",
      " [  0   0   1  19  12   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 507.\n",
      "For a k value = 6 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   0   9  14   2   0   0]\n",
      " [  0   6 178  91  16   0   0]\n",
      " [  0   2 117 265  43   5   0]\n",
      " [  0   2  20 107  63   0   0]\n",
      " [  0   1   3  17  11   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 509.\n",
      "For a k value = 7 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   0   8  15   2   0   0]\n",
      " [  0   3 170 101  17   0   0]\n",
      " [  0   1 104 281  42   4   0]\n",
      " [  0   1  23 108  59   1   0]\n",
      " [  0   1   3  17  10   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 514.\n"
     ]
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong high quality indicators\n",
    "X = X_initial[ ['alcohol', 'density'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  1   0   2   2   0   0   0]\n",
      " [  1   8   8   8   0   0   0]\n",
      " [  3  16 186  72  12   2   0]\n",
      " [  2  24 164 219  21   2   0]\n",
      " [  0   3  79  69  41   0   0]\n",
      " [  0   0  12  11   8   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 459.\n",
      "For a k value = 3 : \n",
      "[[  0   0   3   1   1   0   0]\n",
      " [  1   3  11   9   1   0   0]\n",
      " [  3  14 145 102  22   5   0]\n",
      " [  1  16 122 247  44   2   0]\n",
      " [  0   1  72  65  52   2   0]\n",
      " [  0   0  13  12   5   5   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 452.\n",
      "For a k value = 4 : \n",
      "[[  1   0   2   1   1   0   0]\n",
      " [  1   2  14   7   1   0   0]\n",
      " [  0   8 149 108  25   1   0]\n",
      " [  1   7 127 260  33   4   0]\n",
      " [  0   2  69  73  46   2   0]\n",
      " [  0   0  12  14   4   5   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 463.\n",
      "For a k value = 5 : \n",
      "[[  1   0   3   1   0   0   0]\n",
      " [  1   3  14   7   0   0   0]\n",
      " [  0   9 137 126  18   1   0]\n",
      " [  0   8 134 257  28   5   0]\n",
      " [  0   2  70  81  38   1   0]\n",
      " [  0   0  10  16   6   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 439.\n",
      "For a k value = 6 : \n",
      "[[  1   0   1   2   1   0   0]\n",
      " [  1   3  12   9   0   0   0]\n",
      " [  0   8 138 120  24   1   0]\n",
      " [  0  10 127 264  28   3   0]\n",
      " [  0   3  72  81  35   1   0]\n",
      " [  0   1  12  14   5   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 444.\n",
      "For a k value = 7 : \n",
      "[[  1   0   1   2   1   0   0]\n",
      " [  1   3  12   8   1   0   0]\n",
      " [  0   7 137 119  27   1   0]\n",
      " [  0   9 127 262  33   1   0]\n",
      " [  0   1  61  91  37   2   0]\n",
      " [  0   0  11  17   4   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 443.\n"
     ]
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong low quality indicators\n",
    "X = X_initial[ ['free sulfur dioxide', 'volatile acidity', 'fixed acidity'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a k value = 2 : \n",
      "[[  0   0   3   1   1   0   0]\n",
      " [  0   2  11  11   1   0   0]\n",
      " [  0  27 173  75  15   1   0]\n",
      " [  3  24 171 199  28   7   0]\n",
      " [  3  10  42  86  47   4   0]\n",
      " [  0   1   7  15   9   3   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 424.\n",
      "For a k value = 3 : \n",
      "[[  0   0   2   1   2   0   0]\n",
      " [  0   2   9  12   2   0   0]\n",
      " [  2  23 126 115  22   3   0]\n",
      " [  2  20 141 218  43   8   0]\n",
      " [  3  11  37  73  62   6   0]\n",
      " [  0   1   8  12  10   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 412.\n",
      "For a k value = 4 : \n",
      "[[  0   0   2   2   1   0   0]\n",
      " [  0   1  10  10   3   1   0]\n",
      " [  0  13 147 109  18   4   0]\n",
      " [  0  12 142 230  43   5   0]\n",
      " [  0   6  35  94  52   5   0]\n",
      " [  0   1   7  12  11   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 434.\n",
      "For a k value = 5 : \n",
      "[[  0   0   2   2   1   0   0]\n",
      " [  0   0  10  11   4   0   0]\n",
      " [  0   6 138 130  13   4   0]\n",
      " [  1   6 134 254  33   4   0]\n",
      " [  0   0  32 105  49   6   0]\n",
      " [  0   0   6  15  10   4   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 445.\n",
      "For a k value = 6 : \n",
      "[[  0   0   4   1   0   0   0]\n",
      " [  0   1  10  12   2   0   0]\n",
      " [  0   3 137 138  11   2   0]\n",
      " [  0   5 146 242  35   4   0]\n",
      " [  0   0  33 103  52   4   0]\n",
      " [  0   0   7  18   8   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 434.\n",
      "For a k value = 7 : \n",
      "[[  0   0   3   2   0   0   0]\n",
      " [  0   1  10  11   2   1   0]\n",
      " [  0   1 129 148  12   1   0]\n",
      " [  0   4 129 259  38   2   0]\n",
      " [  0   0  30 107  51   4   0]\n",
      " [  0   0   7  18   8   2   0]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "Sum of the diagonals is: 442.\n"
     ]
    }
   ],
   "source": [
    "#knn classification taking features that seem to be strong all around quality indicators\n",
    "X = X_initial[ ['chlorides', 'residual sugar'] ]\n",
    "y = white_wine['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ks = [2,3,4,5,6,7]\n",
    "\n",
    "for k in ks:    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"For a k value = \" + str(k) + \" : \")\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))\n",
    "    print(\"Sum of the diagonals is: \" + str(np.trace(confusion_matrix(y_test, y_pred, labels=[3, 4, 5, 6, 7, 8, 9]))) + \".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best knn classification so far is a 7 neighbor classifier on the 'alcohol' and 'density' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chlorides</th>\n",
       "      <th>residual sugar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4656</th>\n",
       "      <td>0.048</td>\n",
       "      <td>10.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>0.036</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>0.036</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>0.054</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>0.044</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>0.034</td>\n",
       "      <td>8.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>0.043</td>\n",
       "      <td>11.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>0.032</td>\n",
       "      <td>1.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>0.033</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>0.044</td>\n",
       "      <td>14.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>0.034</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.060</td>\n",
       "      <td>5.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.044</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>0.052</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>0.120</td>\n",
       "      <td>22.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>0.050</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>0.035</td>\n",
       "      <td>5.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>0.038</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>0.028</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4388</th>\n",
       "      <td>0.036</td>\n",
       "      <td>7.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>0.053</td>\n",
       "      <td>12.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>0.047</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4101</th>\n",
       "      <td>0.057</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0.062</td>\n",
       "      <td>9.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.036</td>\n",
       "      <td>11.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>0.051</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>0.053</td>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>0.034</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.044</td>\n",
       "      <td>13.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>0.047</td>\n",
       "      <td>5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>0.065</td>\n",
       "      <td>12.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.049</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.037</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>0.039</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>0.056</td>\n",
       "      <td>13.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>0.038</td>\n",
       "      <td>5.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>0.034</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>0.046</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>0.056</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>0.035</td>\n",
       "      <td>4.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>0.033</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.049</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.025</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>0.043</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.043</td>\n",
       "      <td>11.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.042</td>\n",
       "      <td>11.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.060</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.038</td>\n",
       "      <td>18.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>0.040</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>0.039</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>0.036</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>0.098</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0.050</td>\n",
       "      <td>7.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.057</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>0.055</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>0.045</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>0.028</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>0.034</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.055</td>\n",
       "      <td>8.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>0.049</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>980 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chlorides  residual sugar\n",
       "4656      0.048           10.80\n",
       "3659      0.036            2.70\n",
       "907       0.036            2.10\n",
       "4352      0.054            1.60\n",
       "3271      0.044            5.70\n",
       "4632      0.034            8.20\n",
       "2244      0.043           11.60\n",
       "1924      0.032            1.60\n",
       "3801      0.033            1.10\n",
       "2634      0.044           14.35\n",
       "2827      0.034            1.20\n",
       "79        0.060            5.40\n",
       "350       0.044            2.10\n",
       "1370      0.052            1.90\n",
       "4480      0.120           22.60\n",
       "3783      0.050            8.30\n",
       "2890      0.035            5.15\n",
       "3193      0.038            1.00\n",
       "1281      0.028            1.80\n",
       "4388      0.036            7.10\n",
       "653       0.053           12.30\n",
       "2290      0.047            1.40\n",
       "4101      0.057            9.50\n",
       "1921      0.062            9.30\n",
       "598       0.036           11.40\n",
       "2067      0.051            4.40\n",
       "2794      0.053           10.10\n",
       "3826      0.034            6.80\n",
       "841       0.044           13.40\n",
       "2643      0.047            5.20\n",
       "...         ...             ...\n",
       "1808      0.065           12.40\n",
       "1055      0.049            2.20\n",
       "309       0.037            4.80\n",
       "2370      0.039            6.00\n",
       "3796      0.056           13.90\n",
       "3655      0.038            5.70\n",
       "527       0.034            1.80\n",
       "4114      0.046            1.50\n",
       "1894      0.056            6.50\n",
       "2124      0.035            4.90\n",
       "3598      0.033            1.40\n",
       "378       0.049            2.60\n",
       "495       0.025            7.00\n",
       "2092      0.043            2.70\n",
       "184       0.043           11.90\n",
       "205       0.042           11.60\n",
       "1056      0.060            1.20\n",
       "410       0.038           18.95\n",
       "3776      0.040            7.70\n",
       "3050      0.039            1.70\n",
       "3240      0.036            1.00\n",
       "4089      0.098            4.60\n",
       "1438      0.050            7.10\n",
       "305       0.057            6.80\n",
       "3347      0.055            6.70\n",
       "3207      0.045            1.20\n",
       "1539      0.028            5.00\n",
       "964       0.034           12.50\n",
       "168       0.055            8.95\n",
       "3661      0.049            1.20\n",
       "\n",
       "[980 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
